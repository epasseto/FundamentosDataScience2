{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bom dia, galera! Então, projeto final de machine learning, task 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "então, eu não preciso fazer 'nada' na linha abaixo? Digo, não preciso programar? Eu deixo a seguinte linha nessa task: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_classifier_and_data(clf, my_dataset, features_list)\"\n",
    "Daí o programa roda certinho (o poi_id.py), mas quando rodo o programa tester.py, dá o seguinte erro:\n",
    "\"Traceback (most recent call last):\n",
    " File \"tester.py\", line 109, in <module>\n",
    "   main()\n",
    " File \"tester.py\", line 106, in main\n",
    "   test_classifier(clf, dataset, feature_list)\n",
    " File \"tester.py\", line 37, in test_classifier\n",
    "   for train_idx, test_idx in cv:\n",
    "TypeError: 'StratifiedShuffleSplit' object is not iterable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse problema acontece porque vc está usando uma versão \"muito atualizada\" do Scikit Learn.\n",
    "\n",
    "Após a versão 0.19.1 o `StratifiedShuffleSplit` não pode mais ser \"iterado\".\n",
    "\n",
    "Há duas soluções:\n",
    "\n",
    "1) Vc fazer um _downgrade_ do Scikit Learn, e;\n",
    "2) Alterar o `tester.py`.\n",
    "\n",
    "Sugiro que faça o _downgrade_ para não alterar o `tester.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por favor, acompanhe os números das linhas do repositório para nos situarmos.\n",
    "\n",
    "https://github.com/udacity/ud120-projects/blob/master/final_project/tester.py\n",
    "\n",
    "A primeira coisa é a mudança do nome do módulo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linha 15\n",
    "# De:\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "#Para:\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Falta inserir depois da linha 28 uma nova linha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de como proceder.\n",
    "\n",
    "# Isso é um análogo da linha 28.\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
    "\n",
    "# Isso é o que falta.\n",
    "sss.get_n_splits(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da linha 33 até a 43 vc terá que substituir por:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo genérico que deve ser substituído com os valores reais.\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "   X_train, X_test = X[train_index], X[test_index]\n",
    "   y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que eu usei o próprio exemplo do Scikit Learn. Logo, vc terá que substituir os `x_train`, `x_test`, `y_train` e `y_test` pelo `features_train`, `features_test`, `labels_train` e `labels_test`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O detalhe que ficou obscuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "para mim não funcionou, vou mandar como eu fiz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(clf, dataset, feature_list, folds = 1000):\n",
    "   data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "   labels, features = targetFeatureSplit(data)\n",
    "#    cv = StratifiedShuffleSplit(labels, folds, random_state = 42).get_n_splits(features,labels)\n",
    "#    sss = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "   sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
    "   sss.get_n_splits(features,labels)\n",
    "   #.get_n_splits(features,labels)\n",
    "   true_negatives = 0\n",
    "   false_negatives = 0\n",
    "   true_positives = 0\n",
    "   false_positives = 0\n",
    "   for train_index, test_index in sss.split(features, labels):\n",
    "       features_train, features_test = features[train_index], features[test_index]\n",
    "       labels_train, labels_test = labels[train_index], labels[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dá esse erro: \"Traceback (most recent call last):\n",
    " File \"tester.py\", line 120, in <module>\n",
    "   main()\n",
    " File \"tester.py\", line 117, in main\n",
    "   test_classifier(clf, dataset, feature_list)\n",
    " File \"tester.py\", line 55, in test_classifier\n",
    "   features_train, features_test = features[train_index], features[test_index]\n",
    "TypeError: only integer scalar arrays can be converted to a scalar index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creio que vc não entendeu o que foi explicado, mas vc fez justamente da maneira como era para ser feito.\n",
    "1) Cria-se o objeto `sss`;\n",
    "2) Usa-se o `get_n_splits` nesse objeto criado;\n",
    "3) Por fim, aplica o `split` no `sss`.\n",
    "É a aplicação do que está na documentação do Scikit Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contudo, atente-se ao fato de vc ter configurado valores diferentes para `n_split`, `random_state`, etc daquele do original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "\n",
    "# Ajustado\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usando o original dá o seguinte erro: \"ValueError: test_size=1000 should be smaller than the number of samples 141\n",
    "\". é o folds que na função o default está em 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vc está interpretando incorretamente o `test_size`. Ele deve ser um valor entre 0 e 1 (0 e 100%). Além disso, quando o `test_size` é omitido o _Scikit Learn_ entende que será usado o valor default que é 10% ou 0,1. Já o `n_split` será o \"folds\", OK? Para dirimir a sua dúvida, por favor, leia a Documentação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_size : float, int, None, optional\n",
    "If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.1. The default will change in version 0.21. It will remain 0.1 only if train_size is unspecified, otherwise it will complement the specified train_size.\n",
    "\n",
    "n_splits : int, default 10\n",
    "Number of re-shuffling & splitting iterations\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ainda não entendo o meu erro, eu coloco como na documentação: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o for fica assim: (e dá o erro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in sss.split(features, labels):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File \"tester.py\", line 120, in <module>\n",
    "   main()\n",
    " File \"tester.py\", line 117, in main\n",
    "   test_classifier(clf, dataset, feature_list)\n",
    " File \"tester.py\", line 55, in test_classifier\n",
    "   features_train, features_test = features[train_index], features[test_index]\n",
    "TypeError: only integer scalar arrays can be converted to a scalar index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "de fato, train_index não é inteiro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sss.split(features, labels) aparece como sendo um generator\n",
    "\n",
    "a primeira saída de train_index para mim é: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[138  82   5  18  89  66  12 133  90  65  27 125  22  64  32  13 115 110\n",
    " 83  25 109 127  86 116  20  77  29  80  81  62  10   8   0 134  36  47\n",
    " 52 107  51  72  41  50 137  43  31  78  34  40  79  97 103 102  93  57\n",
    " 74  91  17  46  11  39  49  30 106  37 135 104   4  21  55 117]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bom, parece que ele está gerando os \"índices\" corretamente para fazer o _subset_ de `features` e `labels`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faça um teste, converta o `features` e o `labels` para numpy array (`np.array()`).\n",
    "Se vc fizer isso vai dar certo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "funcionou! mas está demorando para rodar! era para ser rápido?\n",
    "\n",
    "acho que eu tb posso estar errando em como exportei meu clf no poi_id.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demora viu. Se vc optou por usar o AdaBoost, demorará alguns minutinhos para rodar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se vc está com dúvida se o modelo travou, configure o `verbose` para 5, assim a cada etapa o modelo irá imprimir algo e vc terá uma noção se travou ou não"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "só para confirmar, eu coloquei "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf= grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ele vai retornar o modelo que obteve os melhores resultados. Daí pode ser o modelo que obteve a melhor média de score do test dataset, isso dependerá de como vc configurou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sklearn.model_selection.GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, n_jobs=None, iid='warn', refit=True, cv='warn', verbose=0, pre_dispatch='2*n_jobs', error_score='raise-deprecating', return_train_score='warn')[source]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verbose : integer\n",
    "Controls the verbosity: the higher, the more messages.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ah ok! queria só saber se estava certo a maneira de obter o clf do melhor modelo a partir de grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vc pode dar uma olhada em todo o resultado do `GridSearchCV` usando o atributo `cv_results_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna um dicionário com os valores obtidos do `GridSearchCV`.\n",
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além disso, vc pode transformá-lo num pandas para facilitar a visualização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "massa! obrigado :slightly_smiling_face: agora deu tudo certo :slightly_smiling_face: !!!! É que era  pesado o que eu tava usando antes, eu coloquei agora so um gaussian naive bayes e deu o seguinte ao rodar tester.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline(memory=None,\n",
    "    steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
    " svd_solver='auto', tol=0.0, whiten=False)), ('gnb', GaussianNB(priors=None, var_smoothing=1e-09))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy: 0.84789    Precision: 0.40426    Recall: 0.42222    F1: 0.41304    F2: 0.41850\n",
    "    Total predictions:  355    True positives:   19    False positives:   28    False negatives:   26    True negatives:  282"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "antes eu tava rodando, para testar,  o suport vector classifier: e colocava como parametros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc__gamma' : [1,5,10],\n",
    "   'svc__kernel' : ['linear','rbf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "acho que isso era muito pesado!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não se esqueça que o SVM necessita que vc faça o _feature scale_, caso contrário ele vai demorar séculos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o codigo funcionou agora com o SVC, não demora mais séculos hahahahaa é que eu fazia o MinMaxScaler() na \"mao\", o certo é no Pipeline. Eu fazia depois de chamar o Pipeline, daí não adianta\n",
    "\n",
    "agora vem a seguinte coisa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Got a divide by zero when trying out: Pipeline(memory=None,\n",
    "    steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
    " svd_solver='auto', tol=0.0, whiten=False)), ('svc', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    " decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
    " kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
    " shrinking=True, tol=0.001, verbose=False))])\n",
    "Precision or recall may be undefined due to a lack of true positive predicitons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "só que olhe como são as minhas predições usando SVC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    "0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não há problema algum vc aplicar o _feature scale_ mesmo \"não sendo necessário\". Eu faço isso como um preparativo para usar os algoritmos.\n",
    "\n",
    "Isso vai dar problema na hora de calcular as métricas, pois não tem nenhum label do tipo 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entendi! desconfiava mesmo que fosse esse monte de zeros!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vc está usando o `StratifiedShuffleSplit`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estou\n",
    "\n",
    "no tester.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como está o seu `GridSearchCV`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "está assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline, parameters, cv=5,n_jobs=-1, verbose=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que vc necessita fazer algum tipo de _cross_validation_, pode ser um \"manual\" vc criando os datasets de treino e testes ou vc usando o built-in `cv`.\n",
    "\n",
    "Vc está usando um `KFoldsStratified` que é o _default_ do `GridSearchCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ah então, eu não entendi bem o que é esse cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O _cross_validation_ é uma maneira de vc avaliar o seu modelo se ele não vai ficar _overfitted_ ou incorrer em outros tipos de problemas.\n",
    "\n",
    "No caso vc usou `cv = 5`, isto quer dizer que vc usará 5 folds, logo haverá 5 simulações alternando o fold usado como test. No final vc fará uma média dos resultados das métricas. Isso é o que vc está fazendo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estou lendo sobre isso! realmente, eu estava fazendo às cegas, não tinha captado a mensagem do cross-validation! estou com 5 folds. Então, eu deveria alterar esse numero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que o fold significa que vc irá dividir o seu dataset. Vamos fazer contas.\n",
    "1) Vc deve ter 140 observações aproximadamente;\n",
    "2) Caso vc adote `cv = 10`, vc terá  10% de teste sobrará 126 observações para treinar e 14 para teste;\n",
    "3) A proporção de POI e non-POI é de 18/122 (mais ou menos);\n",
    "4) O dataset de teste (14 observações) deverá seguir a mesma proporção 18/122.\n",
    "Logo, se vc opta por muitos `cv` o seu test dataset pode ficar extremamente reduzido e o pior é que ele não conseguirá manter a proporcionalidade. No limite, `cv` pode ser igual ao número de observações.\n",
    "\n",
    "Posto isto, para datasets pequenos aconselha-se o uso do `StratifiedShuffleSplit`, conforme apontado nas instruções do projeto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ah entendi!\n",
    "\n",
    "então, eu deveria mudar, por exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.model_selection import train_test_split\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "   train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "de train_test_split para StratifiedShuffleSplit?\n",
    "\n",
    "como a gente fez no tester.py?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não é necessário. Pode ser mais simples.\n",
    "\n",
    "Diferentemente do `tester.py` onde criamos tudo passo-a-passo. No GridSearchCV vc pode apenas criar o objeto e definir como sendo o `cv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o objeto StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
    "\n",
    "# No seu GridSearchCV\n",
    "objeto = GridSearchCV(XX, XX, XX, cv = sss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vc pode tanto definir um número para o `cv` quanto um objeto do tipo SSS. O `GridSearchCV` é sagaz e entende o que queremos fazer.\n",
    "\n",
    "Todavia, atente-se ao fato de que vc terá que treinar o seu `objeto` usando todo o `features` e `labels`, pois *internamente* o `GridSearchCV` fará para vc o manejo do que será dataset de treino e de testes.\n",
    "\n",
    "Isso pode automatizar demasiadamente e causar confusão, pois parece que estamos esquecendo de alguma coisa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "não entendi essa sua ultima parte. eu faço o cv=sss e dá um erro: ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
    "o que sera que esta havendo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que estranho!\n",
    "\n",
    "Vc só precisa preencher os `...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalmente eu fazia asssim.\n",
    "objeto = GridSearchCV( ... , cv = StratifiedShuffleSplit(n_splits=20, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eu fiz assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=18, test_size=0.1, random_state=0)\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters,n_jobs=-1, verbose=5,cv=sss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e o problema está no fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vc só treina.\n",
    "\n",
    "Depois vc usa o `cv_results_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para visualizar os resultados.\n",
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mas da erro bem no fit\n",
    "\n",
    "não sei se ajuda, mas olha o erro todo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueError: The least populated class in y has only 1 member, which is too few. \n",
    "The minimum number of groups for any class cannot be less than 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seus labels provavelmente estão com algum erro\n",
    "\n",
    "Vc tem que usar o features e labels completos, sem separar em treino e test.\n",
    "\n",
    "Imprima para que possamos ver o conteúdo dos labels, por favor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[array([400000.]), array([350000.]), array([750000.]), array([0.]), array([600000.]), array([600000.]), array([350000.]), array([700000.]), array([0.]), array([600000.]), array([600000.]), array([325000.]), array([0.]), array([800000.]), array([1000000.]), array([1200000.]), array([5249999.]), array([1350000.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([425000.]), array([0.]), array([0.]), array([0.]), array([1000000.]), array([1000000.]), array([800000.]), array([900000.]), array([0.]), array([0.]), array([2500000.]), array([0.]), array([509870.]), array([800000.]), array([2000000.]), array([8000000.]), array([900000.]), array([800000.]), array([5600000.]), array([0.]), array([70000.]), array([0.]), array([600000.]), array([0.]), array([0.]), array([1500000.]), array([7000000.]), array([0.]), array([0.]), array([600000.]), array([1200000.]), array([0.]), array([0.]), array([325000.]), array([0.]), array([3100000.]), array([0.]), array([0.]), array([850000.]), array([1700000.]), array([0.]), array([0.]), array([700000.]), array([650000.]), array([0.]), array([4175000.]), array([400000.]), array([1100000.]), array([1300000.]), array([300000.]), array([500000.]), array([250000.]), array([700000.]), array([1750000.]), array([300000.]), array([250000.]), array([1000000.]), array([0.]), array([0.]), array([0.]), array([3000000.]), array([300000.]), array([1150000.]), array([500000.]), array([700000.]), array([700000.]), array([1100000.]), array([0.]), array([0.]), array([200000.]), array([1000000.]), array([2600000.]), array([0.]), array([750000.]), array([400000.]), array([1000000.]), array([0.]), array([0.]), array([0.]), array([0.]), array([100000.]), array([0.]), array([2000000.]), array([97343619.]), array([0.]), array([750000.]), array([0.]), array([200000.]), array([0.]), array([1500000.]), array([0.]), array([400000.]), array([1250000.]), array([0.])] labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complementando o Anderson, o StratifiedSuffleSplit já faz as divisões entre teste e treino, quando você não adiciona o parâmetro `test_size=x` ele assume que 50% é para treino e 50% para teste\n",
    "\n",
    "Seu labels realmente estão com erro\n",
    "\n",
    "Vc está dando scale nos labels?\n",
    "\n",
    "Os labels deveriam ser apenas `True` e `False`\n",
    "\n",
    "Sim, tô achando que o Vagner deu _scale_ em tudo.\n",
    "\n",
    "Mas pelos valores acredito que não, Vagner você colocou o `'poi'` como primeiro valor na feature_list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é, tá com erro nos labels! coloquei poi sim\n",
    "\n",
    "o incrível é que tava rodando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O poi tem de ficar em primeiro sempre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = ['poi', 'salary', 'deferral_payments', 'total_payments', ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bom, problema identificado!\n",
    "\n",
    "O algoritmo é um moedor de carne. Ele foi feito para rodar! Se as entradas são corretas ou não não importa para ele, pois fará o máximo para te dar um resultado que não seja \"erro\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "na task 1, a feature selection estava retornando:\n",
    "\n",
    "não tava retornando poi\n",
    "\n",
    "é aí o erro né?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['bonus', 'exercised_stock_options', 'shared_receipt_with_poi', 'director_fees', 'deferred_income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sim você tem de modificar sua features_list e manter o poi no inicio e adicionar as demais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consertei! agora meu labels deram isso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([1.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([0.]), array([1.]), array([0.]), array([0.])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " isso ae\n",
    " \n",
    " 0 é nonPOI e 1 é POI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agora eu tenho como resultado ao roda poi_id.py:\n",
    "    \n",
    "mas o tester.py ainda dá o erro lá da divisão por zeros (predictions ainda continua 0). Mas estranho o recall e precision terem aumentado, não?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    "0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "recall 0.8611111111111112 precision 0.8611111111111112"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "você esta usando python 2.7 ou python 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.7. mas acho que o erro está em outra parte pq agora o codigo foi alterado ne, eu tenho que consertar outras partes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso, verifique se o dicionário salvo esta correto e se a lista das features também\n",
    "\n",
    "Esse problema da divisão por zero é complicado.\n",
    "\n",
    "Isso ocorre pelo dataset conter muitos zeros, alguns testes vão fazer a divisão usando esse zero e acabar dando esse erro\n",
    "\n",
    "mas isso depende do resultado do seu modelo tbm, se ele só consegue prever zeros, vai dar esse probleminha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "acho que o problema era a formulação do SVC! pois com GNB deu resultados bem razoaveis! E com AdaBoost da certinho no tester.py, com recall e precision maior do que 0.3.\n",
    "\n",
    "aqui no meu codigo, AdaBoost e KNN não rodam com PCA. É isso mesmo? Se sim, há algum motivo?\n",
    "\n",
    "esquece, era erro meu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "não entendi porque tem o tester.py. Nele é calculado, na \"mão\" precision e recall, certo? Mas isso pode ser calculado no poi_id.py através de:\n",
    "\n",
    "não é? E para mim os resultados dão diferentes, bem diferentes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = recall_score(predict,labels_test, average='micro')\n",
    "prec = precision_score(predict,labels_test, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É devido a quantidade de vezes que você roda, lá é uma media geral de 1000 iterações, você fazendo 1000 iterações e mantendo o StratfiedShuffleSplit com random_state=42 deve gerar um valor bem proximo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ah então o valor deve ser bem proximo? Estranho que para mim tá dando bem diferente\n",
    "\n",
    "No poi_id.py eu escrevo assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = recall_score(predict,labels_test, average='micro')\n",
    "prec = precision_score(predict,labels_test, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e o clf para a task 6 como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf= grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creio que o seu problema seja a configuração do `recall_score`. Entretando, devo ressaltar que isso em termos gerais é um pouco temerário quando acontece, pois se nota que o modelo pode estar pouco \"estável\". Um dos motivos para acontecer isso é o _Data Leakage_. Pode ser até mesmo pouquíssimas simulações no cross_validation, pois isso é um resultado de uma média.\n",
    "\n",
    "Observe ao fato de vc ter definido `average = 'micro'`. Creio que devemos explanar sobre isso:\n",
    "\n",
    "* `macro`: Retorna apenas 1 valor que é a média do recall de cada label (no caso, label zero e label um);\n",
    "* `micro`: Também retorna apenas 1 valor, mas usa um conceito de calcular o total de True Positive e o Total de False Positive para calcular o recall. Observe que no projeto temos label zero e label um, cada label tem o seu recall, precision e accurácia.\n",
    "* `None`: Retorna *dois* valores, o primeiro é o recall do label zero e o segundo do label um."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ah tah, cada label, 0 ou 1, tem o seu recall. logo, seria o mais correto usar a opcao 'None'\n",
    "\n",
    "supondo que o problema de compatibilidade entre os recalls seja por Data Leakage, como eu faria para identificar qual feature esta com muitos NaNs? Força bruta mesmo, contando?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tem uma outra opção quando vc omite o `Average`, daí ele só retorna o recall do label 1. Daí fica a teu critério qual deles usar, mas o modelo usado no `tester.py` está alinhado com o `None` ou quando vc omite o `average`.\n",
    "\n",
    "Vc pode contar, vai ter variável aí com mais de 90% de `NaN`'s, sugiro que vc evite de usá-lo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "ola galera, boa noite! Então, no projeto final de machine learniing, mexendo no grid_search_cv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5,test_size = 0.5, random_state=42)\n",
    "\n",
    "scoring = {'accuracy':make_scorer(accuracy_score),\n",
    "        'precision':make_scorer(precision_score),\n",
    "        'recall':make_scorer(recall_score)}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters,n_jobs=-1, verbose=5,cv=sss,scoring=scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quando eu coloco scoring para o GridSearchCV lidar com as metricas, accuracy, etc, o que esta'faltando? Pq da o seguinte erro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueError: For multi-metric scoring, the parameter refit must be set to a scorer key to refit an estimator with the \n",
    "best parameter setting on the whole data and make the best_* attributes available for that metric. If this is not \n",
    "needed, refit should be set to False explicitly.'AUC' was passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eu coloquei:\n",
    "\n",
    "agora funciona!\n",
    "\n",
    "mas ainda o meu recall do tester.py da diferente do recall que eu calculo no poi_id.py!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline, parameters,n_jobs=-1, verbose=5,cv=sss,scoring=scoring,refit='recall',\n",
    "                           return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vc está usando o `make_scorer` o `GridSearchCV` precisa de uma métrica que vc julga como favorita, pois assim ele conseguirá retornar um melhor estimador quando o `best_estimator_` for usado.\n",
    "\n",
    "A sua preferência será definida no `refit`. Portanto, defina o refit, por exemplo, como `refit = 'recall'`.\n",
    "\n",
    "Faça esse teste e veja se resolve o seu problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "hein galera, boa noite! Agora eu queria que voces me explicassem uma coisa, eu sei que é fora do escopo do curso, mas se alguém puder me elucidar qual a diferença entre: \"Decision Tree\", \"Random Forest\" e \"AdaBoost\", eu agradeceria. É que no projeto final eu gostaria de usar random forest e adaboost, mas não ficou claro para mim cada um deles. Eu sei que eles tem alguma coisa em comum, mas não entendi bem o que é! Vlw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perdoe-me pela demora em responder.\n",
    "\n",
    "Há algoritmos que compartilham um pouco da sua essência. Random Forest e Decision Trees são bem parecidos, pois o Random Forest foi criado baseado na Decision Trees.\n",
    "\n",
    "**Decision Trees**\n",
    "\n",
    "A partir de questões simples cria-se uma árvore de classificação, a sequência das \"perguntas\" é de acordo com o índice de impureza (que pode ser calculado a partir do Gini, por exemplo).\n",
    "\n",
    "**Random Forest**\n",
    "\n",
    "É a aplicação da Decision Trees de forma sistemática (centenas de vezes), mas usando o conceito de _Bootstrapping_ para criar diferentes amostras e consequentemente diferentes árvores de decisão. Observe que nesse algoritmo há a \"randomização\", isto é, nem sempre os resultados serão iguais (vc deverá o configurar o `random_seed`), pois foi feito uma escolha aleatória das amostras (no _bootstrapping_).\n",
    "\n",
    "Portanto, o resultado do algoritmo será baseado em várias árvores e apenas não só por uma (que é o caso do Decision Trees). Há outros detalhes que vou omitir agora, mas caso vc sinta necessidade posso te explicar para um entendimento mais profundo.\n",
    "\n",
    "**AdaBoost**\n",
    "\n",
    "Assim como o Random Forest o AdaBoost tbm é baseado na Decision Trees. A diferença está em:\n",
    "\n",
    "* Será criado uma \"Floresta de Galhos\", isto é, não será feito uma árvore completa como é o caso do Random Forest. A partir desses galhos cria-se uma árvore de classificação;\n",
    "* Cada galho terá um peso diferente no algoritmo, diferentemente do caso do Random Forest em que todas as árvores possuíam o mesmo peso;\n",
    "* A sequência dos galhos para criar a Árvore de Classificação é muito importante, pois pode alterar completamente o resultado final. Note que o AdaBoost usa o conceito de propagação de erro (a partir do `sample_weight`), que é calculado com base nas classificações erradas que ele fez;\n",
    "* Por fim, o _bootstrapping_ é usado para criar os galhos (análogo ao Random Forest). Note que o `sample_weight` é usado nesta etapa para dar mais ênfase nas observações que foram incorretamente classificadas.\n",
    "\n",
    "Bom, tá aí um resumão, só não sei se ficou fácil de entender.\n",
    "\n",
    "Ressalto que o AdaBoost é bem mais complicado que os demais, pois é uma \"evolução\" da Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "ola galera, bom dia! Então, projeto final de machine learning! O meu recall calculado com grid_search dá super bom nos dados de teste, mas quando eu rodo o tester.py dá um valor bem baixo. Alguém poderia me dar uma mao? Não entendendo essa discrepância! Eu tive que alterar o tester.py para usar o StratifiedShuffleSplit do model selection, mas não vejo erros, uma vez também que o @AH Uyekita me ajudou nisso. Por que estaria o recall dando baixo no tester.py? Alguém mais já obteve isso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creio que seja o probleminha do `Average = 'micro'`. Sugiro que vc faça o cálculo na mão imprimindo a _Confusion Matrix_, isso será muito revelador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colocando a opção None, as coisas melhoraram mas quando eu coloco o seguinte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'accuracy':make_scorer(accuracy_score),\n",
    "        'precision':make_scorer(precision_score,average=None),\n",
    "        'recall':make_scorer(recall_score,average=None)}\n",
    "\"\n",
    "da o seguinte erro ao rodar o grid search:\n",
    "\"\n",
    "ValueError: scoring must return a number, got [0.86440678 0.        ] (<type 'numpy.ndarray'>) instead. (scorer=precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5,test_size = 0.5, random_state=42)\n",
    "\n",
    "scoring = {'accuracy':make_scorer(accuracy_score),\n",
    "        'precision':make_scorer(precision_score,average=None),\n",
    "        'recall':make_scorer(recall_score,average=None)}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters,n_jobs=-1, verbose=1,cv=sss,scoring=scoring,refit='recall', \n",
    "                           return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo o projeto está baseado no _recall_ e _precision_ do label um. Talvez seja melhor vc omitir o `average` e só trabalhar com as métricas do label um. Aparentemente o problema é que retornou dois valores ao invés de um, o que é esperado pelo `GridSearchCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se eu omitir o average fica como estava anteriormente. não bate o recall com o do tester.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então, tenta isso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quero somente o segundo elemento da lista que é o recall do label 1.\n",
    "make_scorer(precision_score,average=None)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "não dá certo :disappointed: não suporta índice\n",
    "TypeError: '_PredictScorer' object does not support indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso está correto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'accuracy':make_scorer(accuracy_score),\n",
    "        'precision':make_scorer(precision_score),\n",
    "        'recall':make_scorer(recall_score)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se o seu problema é que os valores obtidos durante o processo de \"modelagem/calibragem\" diferem em muito do `tester.py`.\n",
    "0) Dado que não há problemas de código;\n",
    "1) Verificar as suas variáveis;\n",
    "2) Ajustar os parâmetros;\n",
    "3) Analisar se faz sentido o que vc está calculando com base nas informações obtidas.\n",
    "Note que o processo de criação de um modelo é \"iterativo\" e provavelmente vc terá que repetir alguns passos até atingir o seu melhor.\n",
    "\n",
    "Digamos que vc atingiu a sua primeira iteração, agora basta vc revisar as suas variáveis e consequentemente os parâmetros. Levante perguntas para o seu modelo.\n",
    "1) Estou fazendo o _cross_validation_ corretamente???\n",
    "2) As minhas variáveis são boas!?\n",
    "Infelizmente, esse projeto não é \"linear\" e requer que o aluno revisite perguntas já respondidas (as Tasks 1 ao 6) para buscar alguma falha ou erro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "os labels dos dados no tester e no poi_id.py são diferntes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não, são iguais. Há apenas dois labels 0 e 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vc não está usando o `MinMaxScaler` nos labels né!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra coisa... Qual é a composição da sua lista `features_list`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eu me expressei errado! quando dou len(labels) no poi_id.y da 140, ja no tester.py da 141\n",
    "\n",
    "não, não faço scaling nos labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bom, não vejo como isso poderia travar o seu `test_classifier`, mas a priori deveriam ser iguais. Por acaso vc está atualizando o `dict`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usando o dict onde?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repare que a função `test_classifier` tem como input um `dict`. Esse `dict` é exportado pelo `dump_classifier_and_data`. No Task 3 vc tem que criar o `my_dataset` que será exportado pelo `dump_classifier_and_data` e este por sua vez será usado pelo `test_classifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ah então ta aí! Eu so criava novos features, mas não a partir de um novo data_dict, modifica na mão o features\n",
    "\n",
    "fazia assim ó:\n",
    "\n",
    "é basicamente, fora o 'poi' e 'bonus', definir um novo feature como feature[i]/'bonus'[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_to_bonus(features):\n",
    "    \"\"\"\n",
    "        This function gives the quocient of the best features\n",
    "        relatively to the \"bonus\" feature.\n",
    "        Input(s):\n",
    "                 -features: features of the data\n",
    "        Output(s):\n",
    "                  -new_features: features of the data divided by bonus\n",
    "        Remarks: We do not normalize the first column, which is \"bonus\".\n",
    "        To avoid DivisionByZero error, we replace zeros by the mean!        \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    new_features = []\n",
    "#    features\n",
    "    # calculating the average of the first feature!\n",
    "    avg_0=0\n",
    "    for i in range(len(features)):\n",
    "        avg_0+=features[i][1]\n",
    "    \n",
    "    # iterating through features\n",
    "    for i in range(len(features)):\n",
    "        feat_j=[]\n",
    "        for j in range(5):\n",
    "#            if the the list element is not zero!\n",
    "            if(features[i][1]!=0):\n",
    "                if(j!=1 and j!=0):\n",
    "                    feat_j.append(features[i][j]/features[i][1])\n",
    "                elif(j==0):\n",
    "                    feat_j.append(features[i][j])\n",
    "                else:\n",
    "                    feat_j.append(features[i][1])\n",
    "#            if there is a zero, replace it by the average!\n",
    "            if(features[i][1]==0):\n",
    "                if(j!=1 and j!=0):\n",
    "                    feat_j.append(features[i][j]/avg_0)\n",
    "                elif(j==0):\n",
    "                    feat_j.append(features[i][j])\n",
    "                else:\n",
    "                    feat_j.append(avg_0)\n",
    "                    \n",
    "        new_features.append(np.array(feat_j))\n",
    "        \n",
    "    return new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! Então não se esqueça de criar o `my_dataset` que deve possuir a nova _feature_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "muito obrigado :slightly_smiling_face: agora vou seguir batalhando aqui :slightly_smiling_face:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além disso, o `my_dataset` não deve conter os _outliers_.\n",
    "\n",
    "Pense que o `my_dataset` é o seu banco de dados completamente limpo e com as novas variáveis já inclusas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bem que eu estava achando estranho não ter que exportar, ou algo assim, os dados e features criados heheeheh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consegui!!! recall de 1 e precision de 1. Nada mal!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "só tome cuidado com valores muito bons!!! Isso pode ser um alerta para o _Overfitting_!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "certo! mas esse valor é nos dados de teste mesmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blz!! Tá muito bom os resultados. Isso é usando o `test_classifier`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que a nota será feita a partir do `test_classifier`. Portanto, sugiro que faça várias simulações caso vc tenha optado pelo AdaBoost ou Random Forest para ver se os resultados de cada \"rodada\" do `test_classifier` fiquem próximos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ficaram bem proximas! :slightly_smiling_face: recall e precision de 1 é o resultado do test_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahhhh muleke!! É 10 então! hahahahha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vlw! ufa! ralei peito para fazer isso! Estou muito orgulhoso do que já fiz. Po, complicado mesmo esse projeto. Um cara que não tem muito jeito para programar, não faz isso nem lascando. Botando fé na Udacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
